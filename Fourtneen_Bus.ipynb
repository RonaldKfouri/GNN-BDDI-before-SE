{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNB2CIMoboHKVEurQ9qK/Kc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import the necessary libraries"],"metadata":{"id":"KXP-TeSTZKNM"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import scipy\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, matthews_corrcoef\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","from torch.nn import Sequential as Seq, Linear, ReLU\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import torch_geometric\n","from torch_geometric.nn import global_mean_pool, MessagePassing\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.utils import add_self_loops\n","from torch.cuda.amp import GradScaler, autocast\n","\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import gc\n","import signal\n","import sys"],"metadata":{"id":"C56jqLoeZIPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checkpoint and HPC Timeout Handling\n","\n","Since this project was trained on a High-Performance Computing (HPC) cluster, it was necessary to handle job time limits imposed by the compute partitions:\n","\n","- `gpu` partition: maximum runtime of 6 hours  \n","- `large` partition: maximum runtime of 24 hours\n","\n","Training the GNN for the IEEE 14-bus system (and larger test systems) often exceeds these limits.  \n","To prevent job termination and loss of progress, this section defines two key functions:\n","\n","- `save_checkpoint()` â€” saves the model state, optimizer state, training losses, and accuracies to a file.  \n","- `handle_timeout()` â€” automatically triggers a checkpoint save and exits gracefully just before the job time expires.\n","\n","By setting an alarm signal (`signal.alarm`), a checkpoint is saved 5 minutes before the time limit, allowing training to resume later from the saved state rather than restarting from scratch.\n","\n","> Note:  \n","> This cell is specific to the HPC environment used in this study. If you are running the notebook on a different system (e.g., local machine or cloud runtime), you have to modify or remove this section as needed.\n"],"metadata":{"id":"mmhf4MEQqA5U"}},{"cell_type":"code","source":["# Define the save_checkpoint function\n","def save_checkpoint(epoch, model, optimizer, train_losses, val_losses, train_accuracies, val_accuracies, filename='foo14N.chkp'):\n","  \"\"\"\n","  I saved it as foo14N.chkp because the one in the HPC documentation was called foo.chkp,\n","  14 stands for 14-bus, and N stands for No_PCA (as opposed to with_PCA as stated in the paper;\n","  this is the only ambiguous 'encoding' I promise :D)\n","  \"\"\"\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'train_losses': train_losses,\n","        'val_losses': val_losses,\n","        'train_accuracies': train_accuracies,\n","        'val_accuracies': val_accuracies\n","    }\n","    torch.save(checkpoint, filename)\n","    print(f\"Checkpoint saved to {filename}\")\n","\n","# Define the handle_timeout function\n","def handle_timeout(signum, frame):\n","    \"\"\"\n","    This function is called when the alarm goes off\n","    It saves a checkpoint and exits the program\n","    \"\"\"\n","    print(\"Time limit reached. Saving checkpoint and exiting...\")\n","    save_checkpoint(epoch, model, optimizer, train_losses, val_losses, train_accuracies, val_accuracies, 'foo14N.chkp')\n","    sys.exit(0)  # exit the program\n","\n","# Set the signal handler for the alarm signal\n","signal.signal(signal.SIGALRM, handle_timeout)\n","\n","# Set the alarm to go off 5 minutes before the job's time limit (6 hours for `gpu` partition OR 24 hours for the `large` partition)\n","signal.alarm(6 * 60 * 60 - 5 * 60)"],"metadata":{"id":"9B7NgKNPZYjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GPU Setup and Dataset Loading\n","\n","This section verifies GPU availability and prepares the input data for training the GNN model.\n","\n","- Dataset Loading:  \n","  Reads the node features, edge features, and output labels directly from Parquet files.  \n","  These files correspond to the IEEE 14-bus system and were generated separately using MATLAB scripts.\n","\n","- Tensor Conversion and Splitting:  \n","  Converts the loaded data into PyTorch tensors and transfers them to the GPU memory.  \n","  The dataset is then split into training (60%), validation (20%), and testing (20%) sets â€” maintaining the same order across node, edge, and output tensors.\n","\n","- Reshaping:  \n","  Node features are reshaped to `(samples, 14, 2)` representing 14 nodes with 2 features each,  \n","  while edge features are reshaped to `(samples, 20, 2)` representing 20 directed edges with 2 features each.\n","\n","- Memory Management:  \n","  After splitting, temporary variables are deleted and GPU memory is cleared to optimize performance.\n","\n","> Note:  \n","> If you are using a different system, dataset, or bus configuration, update the file paths and reshape dimensions accordingly."],"metadata":{"id":"mXDcuMlfsdAA"}},{"cell_type":"code","source":["# Check for GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Read parquet files (parquet are probably the most efficient (in terms of memory size) for large numerical data)\n","Input_data_frame = pd.read_parquet('~/Your_Path/Node_Features_case14.parquet', engine=\"pyarrow\")\n","df_numbers = pd.read_parquet('~/Your_Path/Output_Labels_Encoded_case14.parquet', engine=\"pyarrow\")\n","Edge_data_frame = pd.read_parquet('~/Your_Path/Edge_Features_case14.parquet', engine=\"pyarrow\")\n","\n","# Convert dataframes to GPU tensors\n","Input_tensor = torch.tensor(Input_data_frame.values, dtype=torch.float16, device=device)\n","df_numbers_tensor = torch.tensor(df_numbers.values, dtype=torch.long, device=device)  # labels are integers\n","Edge_tensor = torch.tensor(Edge_data_frame.values, dtype=torch.float16, device=device)\n","\n","# Split data into training, validation, and test sets (directly using tensors)\n","X_train, X_temp, Y_train, Y_temp = train_test_split(Input_tensor, df_numbers_tensor, train_size=0.6, shuffle=False)\n","X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, train_size=0.5, shuffle=False)\n","\n","Edge_train, Edge_temp, _, _ = train_test_split(Edge_tensor, df_numbers_tensor, train_size=0.6, shuffle=False)\n","Edge_val, Edge_test, _, _ = train_test_split(Edge_temp, Y_temp, train_size=0.5, shuffle=False)\n","\n","# Reshape (on GPU)\n","X_train = X_train.view(-1, 14, 2)\n","X_val = X_val.view(-1, 14, 2)\n","X_test = X_test.view(-1, 14, 2)\n","\n","Edge_train = Edge_train.view(-1, 20, 2)\n","Edge_val = Edge_val.view(-1, 20, 2)\n","Edge_test = Edge_test.view(-1, 20, 2)\n","\n","Y_train = Y_train.squeeze()\n","Y_val = Y_val.squeeze()\n","Y_test = Y_test.squeeze()\n","\n","# Print shapes to verify\n","print(\"X_train Shape:\", X_train.shape)\n","print(\"X_val Shape:\", X_val.shape)\n","print(\"X_test Shape:\", X_test.shape)\n","\n","print(\"Edge_train Shape:\", Edge_train.shape)\n","print(\"Edge_val Shape:\", Edge_val.shape)\n","print(\"Edge_test Shape:\", Edge_test.shape)\n","\n","del Input_data_frame, df_numbers, Edge_data_frame\n","del Input_tensor, df_numbers_tensor, Edge_tensor\n","del X_temp, Y_temp, Edge_temp\n","\n","# Clear GPU memory\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"6QLYIpccZdVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Graph Construction\n","\n","The following code defines the graph connectivity of the IEEE 14-bus system.  \n","The `edge_index` tensor encodes the directed edges of the power network in the format expected by PyTorch Geometric, where each column represents a connection between a source node and a target node.\n","\n","- The first row lists source nodes, and the second row lists target nodes.  \n","- The indexing follows 0-based numbering, consistent with standard graph theory and PyTorch Geometric conventions (e.g., Bus 1 â†’ index 0).\n","\n","> Note:  \n","> For larger test systems (IEEE 33-bus, 118-bus, or 300-bus), manually defining `edge_index` is impractical.  \n","> In such cases, you can write a short MATLAB script to automatically extract the bus connectivity from the system data and export the edge indices in a format compatible with this code."],"metadata":{"id":"Nghd7mil0MHk"}},{"cell_type":"code","source":["edge_index = torch.tensor([\n","    [0,0,1,1,1,2,3,3,3,4,5,5,5,6,6,8,8,9,11,12],  # Source nodes\n","    [1,4,2,3,4,3,4,6,8,5,10,11,12,7,8,9,13,10,12,13]   # Target nodes\n","], dtype=torch.long).to(device)"],"metadata":{"id":"SEshBhudZkqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Preparation and PyTorch Geometric Loaders\n","\n","This section organizes the node, edge, and label tensors into graph objects compatible with the PyTorch Geometric framework.\n","\n","- `create_data_list()` function:  \n","  Iterates over all samples to create a list of `Data` objects, where each object contains the node feature matrix, graph connectivity, edge feature matrix, and the output label (class).\n","\n","- Dataset creation:  \n","  Builds separate datasets for training, validation, and testing, using the splits defined earlier.\n","\n","- Data loaders:  \n","  The `DataLoader` utility enables mini-batch processing of multiple small graphs during training and evaluation.  \n","  \n"],"metadata":{"id":"DmpDEYgP5oEx"}},{"cell_type":"code","source":["def create_data_list(X, Y, edge_features, edge_index):\n","    data_list = [Data(x=X[i], edge_index=edge_index, edge_attr=edge_features[i], y=Y[i]) for i in range(len(X))]\n","    return data_list\n","\n","# Create datasets\n","train_data_list = create_data_list(X_train, Y_train, Edge_train, edge_index)\n","val_data_list = create_data_list(X_val, Y_val, Edge_val, edge_index)\n","test_data_list = create_data_list(X_test, Y_test, Edge_test, edge_index)\n","\n","# Loaders\n","train_loader = DataLoader(train_data_list, batch_size=64, shuffle=False)\n","val_loader = DataLoader(val_data_list, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_data_list, batch_size=64, shuffle=False)"],"metadata":{"id":"TF_Hk9uYZnGB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Edge-Conditioned Convolution Layer\n","\n","This section defines the Edge-Conditioned Convolution (ECC) layer, a custom message-passing neural network block that dynamically adapts node updates based on both neighboring node features and edge attributes.\n","\n","- Core Idea:  \n","  Each message passed between connected nodes depends on the source and target node features and on the properties of the edge connecting them, according to the following equation:   \n","  $\\mathbf{h}_i^{(k+1)} = \\sigma \\Bigg( \\underset{j\\in \\mathcal{N}(i)\\cup\\{i\\}}{\\Psi} \\bigg( f_\\omega \\Big(\\mathbf{h}_i^{(k)}, \\mathbf{h}_j^{(k)}, \\mathbf{e}_{ij}^{(k)} \\Big) \\bigg) \\Bigg)$  \n","\n","- Implementation Details:  \n","\n","  - A small MLP (`self.mlp`) combines the concatenated features `[x_i, x_j, edge_attr]` to compute the messages.  \n","  $\\mathbf{m}_{i \\gets j}^{(k)}=f_{\\omega} \\Big(\\mathbf{h}_i^{(k)} \\| \\mathbf{h}_j^{(k)} \\| \\mathbf{e}_{ij}^{(k)} \\Big)$; ($f_\\omega$ in this case is the MLP)\n","\n","  - Self-loops are automatically added using `add_self_loops`, and zero-valued attributes are created for them to ensure dimensional consistency.  \n","  $\\mathbf{m}_{i \\gets i}^{(k)}=f_{\\omega} \\Big(\\mathbf{h}_i^{(k)} \\| \\mathbf{h}_i^{(k)} \\| \\mathbf{e}_{ii}^{(k)} \\Big)$   \n","\n","  - The layer inherits from `torch_geometric.nn.MessagePassing` with aggregation mode set to `'mean'`.  \n","  $\\mathbf{h}_i^{(k+1)}=\\sigma \\Bigg(\\frac{1}{|\\mathcal{N}(i)|} \\underset{j\\in \\mathcal{N}(i)\\cup\\{i\\}}{\\sum}\\mathbf{m}_{i \\gets j}^{(k)} \\Bigg)$\n","\n","  - The operations involving self-loop attributes are performed in CPU memory first, then safely moved to GPU to avoid device conflicts.\n","\n","> Note:  \n","> The input dimensions (`in_channels_node`, `in_channels_edge`, and `out_channels`) should match your dataset configuration.  \n","> For other network sizes or feature definitions, these values can be easily modified."],"metadata":{"id":"TM0XE-Jo6eJi"}},{"cell_type":"code","source":["class EdgeConditionedConv(MessagePassing):\n","    def __init__(self, in_channels_node, in_channels_edge, out_channels):\n","        super(EdgeConditionedConv, self).__init__(aggr='mean')\n","\n","        self.mlp = torch.nn.Sequential(\n","            torch.nn.Linear(2 * in_channels_node + in_channels_edge, out_channels),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(out_channels, out_channels)\n","        )\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        # Use torch.no_grad() to avoid tracking gradients when modifying edge attributes\n","        with torch.no_grad():\n","            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n","\n","            # Create self-loop attributes in CPU first, then move to GPU\n","            loop_attr = torch.zeros((x.size(0), edge_attr.size(1)), device='cpu', dtype=edge_attr.dtype)\n","            edge_attr = torch.cat([edge_attr.cpu(), loop_attr], dim=0).to(edge_attr.device, non_blocking=True)\n","\n","        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n","\n","    def message(self, x_i, x_j, edge_attr):\n","        return self.mlp(torch.cat([x_i, x_j, edge_attr], dim=-1))"],"metadata":{"id":"N6-cAhdxZpn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Architecture: EC-GNN for Graph-Level Classification\n","\n","This module stacks two Edge-Conditioned Convolution (ECC) layers followed by global mean pooling and two fully connected (FC) layers to produce graph-level predictions (one label per sample).\n","\n","Flow\n","1. ECC layers (`conv1`, `conv2`)  \n","   - Each layer updates node embeddings using messages that depend on both neighbor node features and edge attributes.  \n","   - Nonlinearity: LeakyReLU (0.01) after each ECC.\n","\n","2. Global mean pooling  \n","   - `global_mean_pool(x, batch)` aggregates node embeddings into a single graph embedding per sample.  \n","   $\\mathbf{h}_{\\mathcal{G}}=\\frac{1}{|\\mathcal{V}|} \\sum_{i\\in\\mathcal{V}} \\mathbf{h}_i^{(k+1)}$ you can swap `global_mean_pool` for `global_add_pool` or `global_max_pool` to test alternative aggregations.\n","\n","3. MLP head (`fc1`, `fc2`)  \n","   - Reduces the pooled embedding and maps to `out_channels` logits (number of classes).  \n","\n","\n","Notes\n","- Device placement is enforced at the start of `forward` to avoid CPU/GPU mismatches.  \n","  \n","- The number of hidden layers and the number of neurons per hidden layer were chosen using grid search, and the results were used here.\n","\n"],"metadata":{"id":"JkkqIN0W9ehw"}},{"cell_type":"code","source":["class ECGNN(nn.Module):\n","    def __init__(self, in_channels_node, in_channels_edge, hidden_channels, out_channels):\n","        super(ECGNN, self).__init__()\n","\n","        self.conv1 = EdgeConditionedConv(in_channels_node, in_channels_edge, hidden_channels)\n","        self.conv2 = EdgeConditionedConv(hidden_channels, in_channels_edge, hidden_channels)\n","\n","        self.fc1 = nn.Linear(hidden_channels, hidden_channels // 2)\n","        self.fc2 = nn.Linear(hidden_channels // 2, out_channels)\n","\n","    def forward(self, x, edge_index, edge_attr, batch):\n","        # Ensure data is on the correct device before computing\n","        x, edge_index, edge_attr, batch = x.to(device, non_blocking=True), edge_index.to(device, non_blocking=True), edge_attr.to(device, non_blocking=True), batch.to(device, non_blocking=True)\n","\n","        # Graph convolutions\n","        x = self.conv1(x, edge_index, edge_attr)\n","        x = F.leaky_relu(x, negative_slope=0.01)\n","\n","        x = self.conv2(x, edge_index, edge_attr)\n","        x = F.leaky_relu(x, negative_slope=0.01)\n","\n","        # Global mean pooling\n","        x = global_mean_pool(x, batch)\n","\n","        # Fully connected layers\n","        x = F.leaky_relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x"],"metadata":{"id":"KU5vV7t4Zspy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training Setup and Mixed Precision Optimization\n","\n","This section initializes the EC-GNN model, optimizer, and loss function, and defines the training loop.\n","\n","- Device selection:  \n","  The model automatically runs on GPU if available; otherwise, it falls back to CPU.\n","\n","- Model configuration:*\n","  The `ECGNN` is instantiated with:\n","  - `in_channels_node = 2` â†’ two node features (voltage magnitude and angle OR real and reactive power injection)  \n","  - `in_channels_edge = 2` â†’ two edge features (real and reactive power flow OR current magnitude and angle)  \n","  - `hidden_channels = 128` â†’ hidden dimensionality for learned embeddings. The number of hidden layers and the number of neurons per hidden layer were chosen using grid search, and the results were used here.  \n","  - `out_channels = 35` â†’ number of output classes (for the extended case; adjust as needed)\n","\n","- Optimizer and loss:  \n","  Uses the Adam optimizer with a learning rate of 0.001 and `CrossEntropyLoss`.\n","\n","- Mixed Precision Training (AMP):  \n","  - Implemented via `torch.cuda.amp.GradScaler()` and `autocast()` to reduce memory usage and accelerate training on GPUs without sacrificing accuracy.  \n","  - The scaled gradients are safely unscaled and updated after each batch.\n","\n","- Training function (`train`)  \n","  Performs one full epoch over the training set:\n","  1. Sets the model to training mode (`model.train()`).\n","  2. Iterates over mini-batches from `train_loader`.\n","  3. Computes the loss and gradients using AMP for stability.\n","  4. Tracks the total loss and batch-level accuracy.\n","  5. Explicitly deletes batch tensors after use to free GPU memory.\n","\n","> Note:  \n","> If you are using a smaller dataset or running locally, you can disable mixed precision by removing the `autocast()` context and `GradScaler`.  "],"metadata":{"id":"s8j43QZtSXVp"}},{"cell_type":"code","source":["# Define device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Model, Optimizer, Loss Function\n","model = ECGNN(in_channels_node=2, in_channels_edge=2, hidden_channels=128, out_channels=35).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Gradient Scaler for Mixed Precision Training\n","scaler = GradScaler()\n","\n","def train(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total_samples = 0\n","\n","    for data in train_loader:\n","        data = data.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad()\n","        with autocast():\n","            output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n","            loss = criterion(output, data.y)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item() * data.num_graphs\n","        correct += (output.argmax(dim=1) == data.y).sum().item()\n","        total_samples += data.y.size(0)\n","\n","        del data  # Free up GPU memory\n","\n","    accuracy = correct / total_samples\n","    return total_loss / len(train_loader.dataset), accuracy"],"metadata":{"id":"XpVrF3K1gib8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation and Checkpoint Loading\n","\n","This section defines the evaluation function and the mechanism to load training checkpoints, allowing model training to resume after HPC time limits.  \n","\n","The `evaluate()` function performs validation or testing on a given dataset without updating model weights.\n","\n","- The model is switched to evaluation mode (`model.eval()`), disabling dropout and batch normalization updates.\n","\n","- Inference with AMP: uses `torch.cuda.amp.autocast()` to perform inference in mixed precision, maintaining consistency with training while improving speed and memory efficiency.\n","\n","- Loss and Accuracy:  \n","  For each batch:\n","  - Computes predictions and loss.\n","  - Counts correctly classified graphs.\n","  - Aggregates results over all samples.  \n","  Returns the average loss and accuracy over the dataset.\n","\n","- Memory Efficiency: each batch tensor is deleted after use, freeing GPU memory.\n","\n","Checkpoint Loading and Resume  \n","Since GNN training can exceed the maximum allowed runtime on HPC clusters, this notebook supports resuming from a saved checkpoint.\n","\n","> Note:  \n","> - The checkpoint file name can be changed if running multiple experiments.  \n","> - Ensure that the same model architecture and optimizer settings are used when resuming training."],"metadata":{"id":"OzxfTSjSelVY"}},{"cell_type":"code","source":["def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device, non_blocking=True)\n","\n","            with autocast():  # Enable Mixed Precision for inference\n","                output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n","                loss = criterion(output, data.y)\n","\n","            total_loss += loss.item() * data.num_graphs\n","            correct += (output.argmax(dim=1) == data.y).sum().item()\n","            total_samples += data.y.size(0)\n","\n","            del data  # Free up GPU memory\n","\n","    accuracy = correct / total_samples\n","    return total_loss / len(loader.dataset), accuracy\n","\n","# Define the load_checkpoint function\n","def load_checkpoint(filename='foo14N.chkp'):\n","    if os.path.exists(filename):\n","        checkpoint = torch.load(filename)\n","        print(f\"Checkpoint loaded from {filename}\")\n","        return checkpoint\n","    else:\n","        print(\"No checkpoint found. Starting from scratch.\")\n","        return None\n","\n","# Load checkpoint if it exists\n","checkpoint = load_checkpoint('foo14N.chkp')\n","\n","if checkpoint:\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n","    train_losses = checkpoint['train_losses']\n","    val_losses = checkpoint['val_losses']\n","    train_accuracies = checkpoint['train_accuracies']\n","    val_accuracies = checkpoint['val_accuracies']\n","else:\n","    start_epoch = 0\n","    train_losses, val_losses = [], []\n","    train_accuracies, val_accuracies = [], []"],"metadata":{"id":"5iqp8vAAgmeT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training & Validation Loop + Final Test\n","\n","This section runs the main **epoch loop** with timing, periodic checkpointing, and a final **held-out test** evaluation.  \n","\n","After training completes, runs `evaluate()` on the test set and prints Test Loss and Test Accuracy on the unseen split.\n","\n","Accuracy, F1, and MCC are computed to evaluate the performance of the GNN. MCC is epecially helpful when the dataset is imbalanced.\n"],"metadata":{"id":"hYGSNyrQfwlk"}},{"cell_type":"code","source":["# Training Loop\n","num_epochs = 100\n","start_time = time.time()  # Start timing\n","\n","for epoch in range(start_epoch, num_epochs):\n","    epoch_start = time.time()\n","\n","    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    train_accuracies.append(train_accuracy)\n","    val_accuracies.append(val_accuracy)\n","\n","    epoch_time = time.time() - epoch_start\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_accuracy:.6f}, Val Accuracy: {val_accuracy:.6f}, Time: {epoch_time:.2f} sec')\n","\n","    # Save checkpoint every 5 epochs (or any interval you prefer)\n","    if (epoch + 1) % 5 == 0:\n","        save_checkpoint(epoch, model, optimizer, train_losses, val_losses, train_accuracies, val_accuracies, 'foo14N.chkp')\n","\n","total_time = time.time() - start_time\n","print(f\"\\nTotal training time for {num_epochs} epochs: {total_time:.2f} sec\")\n","\n","\n","# Testing Loop\n","test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n","print(f\"Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\")"],"metadata":{"id":"uBhtzcA2gpaw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Final Evaluation on Test Data\n","\n","After training and validation are complete, this section performs the final evaluation of the trained EC-GNN model on the unseen test set.  \n","Although `evaluate()` was used during training to track progress via loss and accuracy, `evaluate_with_metrics()` provides a comprehensive, publication-ready performance summary.\n","\n","It is used only once after training on the test set to quantify the modelâ€™s generalization ability. It computes additional metrics that are more informative than accuracy alone, particularly for imbalanced classes.\n","\n","> Note:  \n","> - This evaluation is separate from the validation steps inside the training loop.  \n","> - It provides the final quantitative performance of the trained model on completely unseen data."],"metadata":{"id":"XkGcl01cgkj0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdwwADbiXxpl"},"outputs":[],"source":["def evaluate_with_metrics(model, loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total_samples = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device, non_blocking=True)\n","\n","            with autocast():  # Enable Mixed Precision for inference\n","                output = model(data.x, data.edge_index, data.edge_attr, data.batch)\n","                loss = criterion(output, data.y)\n","\n","            total_loss += loss.item() * data.num_graphs\n","            preds = output.argmax(dim=1)\n","            correct += (preds == data.y).sum().item()\n","            total_samples += data.y.size(0)\n","\n","            # Collect predictions and labels for F1 and MCC\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(data.y.cpu().numpy())\n","\n","            del data  # Free up GPU memory\n","\n","    accuracy = correct / total_samples\n","    avg_loss = total_loss / len(loader.dataset)\n","\n","    # Calculate F1 score and MCC\n","    f1 = f1_score(all_labels, all_preds, average='weighted')  # Use 'weighted' for multi-class\n","    mcc = matthews_corrcoef(all_labels, all_preds)\n","\n","    return avg_loss, accuracy, f1, mcc\n","\n","# Testing Loop with F1 and MCC\n","test_loss, test_accuracy, test_f1, test_mcc = evaluate_with_metrics(model, test_loader, criterion, device)\n","print(f\"Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}, Test F1 Score: {test_f1:.6f}, Test MCC: {test_mcc:.6f}\")"]},{"cell_type":"markdown","source":["### ðŸ’¾ Saving the Trained Model\n","\n","Once training and final evaluation are complete, the trained EC-GNN model is saved for future use.  \n","This allows you to reload the learned weights without retraining the model from scratch.\n","\n","- What is saved:  \n","  Only the model parameters (`state_dict`) are stored, not the optimizer or training history.  \n","  This compact format is ideal for inference or transfer learning on similar datasets.\n","\n","> Tip:  \n","> To reload the model later:\n","> ```python\n","> model = ECGNN(in_channels_node=2, in_channels_edge=2, hidden_channels=128, out_channels=35)\n","> model.load_state_dict(torch.load('~/Your_Path/Fourteen_Bus_No_PCA.pth'))\n","> model.eval()\n","> ```\n"],"metadata":{"id":"bWDupqVyhWbo"}},{"cell_type":"code","source":["# Save the trained model\n","model_save_path = os.path.expanduser('~/Your_Path/Fourteen_Bus_No_PCA.pth')\n","torch.save(model.state_dict(), model_save_path)\n","print(f\"Model saved to {model_save_path}\")"],"metadata":{"id":"ipz13iSigsKp"},"execution_count":null,"outputs":[]}]}